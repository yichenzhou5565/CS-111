NAME: Yichen Zhou
EMAIL: yichenzhou@g.ucla.edu
ID: 205140928

Included files:
README: What you are reading now
Makefile: default, graphs, dist, profile, tests
SortedList.h: header file containing function prototypes of various
	      list operations
SortedList.c: Implementations of the header file, respectively.
	      Note the general design of the linked list is just like
	      that in P2A, i.e., a doubly linked list with dummy node
lab2_list.c: Driver. Specifies options, etc.
lab2_list.gp: Plot corresponding graphs
The png files: Graphs produced by make graphs



2.3.1
I believe most cycles in 1 and 2 thread list tests are spent on 
real operations from the SortedList.c file since switches accross
the threads happens much less frequently.

I believe this is the most expensive part of the code because we
spent most time on it and there are more operations reflected.

In high-thread spin-lock test, most cycles are being spent on
spinning to wait for the lock (i.e., access to critical section).

In high-thread mutex-lock test, most cycles are spent on the 
lock/unlock operations, which might be potentially costly.

2.3.2
When the spin-lock version of the list exerciser is run with a large number
of threads, the code that spends the most cycles are the setting lock
code (i.e., the while loop that keep spinning around).

This operation becomes so costly when the number of threads is large because
the larger the number of threads, more threads want to access the critical
section, so each of them have to wait for longer for the lock to be released,
and this is often reflected on extra spinning cycles.

2.3.3
The average lock-wait time rise dramatically with the number of threads because
as the # of threads becomes larger, more and more threads are waiting for
the single lock. In this case, much more number of threads are waiting, which
results in much higher 'overhead' on the waiting time.

The completion time per operation rise less dramatically because though we
have to wait for the lock to be release longer, there are more operations
of lists as well. This compensates, in some degree, the overhead of waiting 
times (averages out in some sense), so the rise of total time per operation
rise less dramatically than the waiting time.

It is possible to have higher average waiting time compared to average time
per operation. Consider the case where multiple threads are waiting for a lock,
where all of those waiting threads will contribute to the avergae waiting time.
On the other hand, time per operation do not have such concurrent laps, therefore
it is possible to have a higher waiting time comparatively.

2.3.4




References:
https://wiki.geany.org/howtos/profiling/gperftools
https://stackoverflow.com/questions/37685434/problems-with-using-gperftools-on-mac-os-x

I installed gperftools on lnxsrv
Here I'll summarize the steps of installing and starting gperftools, for my own
future references:
1. Get the newest release of gperftools from HERE:
   https://github.com/gperftools/gperftools/releases
2. unzip the tarball
   $ tar -xvzf *.tar.gz
3. Change directory:
   $ cd gperftools-2.7
4. Configuration:
   $ ./configure
5. Compilation:
   $ make
6. Installation:
   $ make install
7. Try to find where the .so is located:
   $ cd /lib64
   $ ls libprof*
   ##The output shows the .so for profiling is indeed here

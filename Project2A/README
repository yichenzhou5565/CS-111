NAME: Yichen Zhou
EMAIL: yichenzhou@g.ucla.edu
UID: 205140928

Included files:
lab2_add.c: Share variable and protections
lab2_list.c: Driver for linked list and protections
SortedList.h: Header file for linked list functions
SortedList.c: Implement functions declared in the header
lab2_add.gp: Plot the shared variable version
lab2_list.gp: Plot the linked list version
Makefile: Compile, distribution, test(>200), clean, graph
README: What you are reading now
the .png files: produced by .gp

2.1.1
It takes quite a few iterations for error to occur because it 
we are able to finish executing the program in a very short
time, the other threads will not yet be able to to take over.
In this case, threads will run one after another (i.e., sequentially)
and race condition will not appear.

For the same reasoning, small # of iterations so seldom fail.
Becasue for small # of iterations, race condition will not occur

2.1.2
The --yield runs much slower because switching have overhead

The additional time is contributed to the overhead of switches.

I don't think getting a valid per-operation time for --yield is possible.
Notice that users cannot decide how fast the systen switches
and how much overhead is involved.

2.1.3
Average cost per iteration drops with increasing iterations because
average overhead drops. Notice that iterations does not have much overhead,
but creating threads does. The more time it takes to iterate, the
less significant the overhead is (i.e., average cost drop).

If cost periteration is a function of iteration #, we can choose the
point with lowest cost, i.e., converge.

2.1.4
All operations perform similarly for low # of threads because
as long as there are not so many threads, the cost of lock
overhead will remain low as well.

The three protected options slow down when the # of threads rises
because if the overhead of locks and unlocks goes up with the
number of threads.

2.2.1
Variation in time:
For both graphs, the cost per operation goes up as the number of threads
increase. This might be due to the overhead of locks and protections.

General Shape:
The general shape of the curve is monotonically increasing. This is to say,
cost is positively related with the number of threads. The reason for
this might be that the overhead of locks and unlocks (as in my implementaion
of the driver is pretty frequent) increases with the number of threads 
significantly.

Relative Rates:
The relative increase rate for Part 2 is bigger because we can see 
the curve in the plot is steeper (i.e., bigger slope, increase more
sharply). This is because in my implementation for Part 2, every time
I use the functions declared in the header (if --sync=m or --sync=s),
I would do the lock/unlock stuff. The overhead of this is significant.
I realized after finishing the project that, indeed, it might be possible
to just lock/unlock at the very beginning/end. This would reduce the 
overhead quite a bit, I think. However, I'll not do the modification since
this project is due soon LOL.

2.2.2
Almost the same as mutex.
Variation in time:
For both graphs, the cost per operation goes up as the number of threads
increase. This might be due to the overhead of locks and protections.

General Shape:
The general shape of the curve is monotonically increasing. This is to say,
cost is positively related with the number of threads. The reason for
this might be that the overhead of set/release (as in my implementaion
of the driver is pretty frequent) increases with the number of threads	
significantly.

Relative Rates:
Interestingly, I wouldn't say there is a big difference in tthe relative rates.
Yes they differ, but not much. My guess to this phenomenon is that the 
overhead for spin lock is similar in both cases, since they will be 
spinning around anyway.



=============================================================================
============== Below is for my own references ===============================
=============================================================================







References:
#include <time.h>
int clock_gettime(clockid_t clk_id, struct timespec* tp);
struct timespec{
       time_t tv_sev;
       long tv_nsec; //nanoseconds
}
referred from:
http://man7.org/linux/man-pages/man2/clock_gettime.2.html

#include <pthread.h>
int pthread_create(pthread_t *restrict thread, const pthread_attr_t *restrict attr, 
    void *(*start_routine)(void*), void *restrict arg);
RC=0: success
RC=errno: otherwise
referred from: 
https://computing.llnl.gov/tutorials/pthreads/man/pthread_create.txt

int pthread_mutex_init(pthread_mutex_t *restrict mutex, 
    const pthread_mutexattr_t *restrict attr);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
int pthread_mutex_destroy(pthread_mutex_t *mutex); //delete mutex object
RC=0: success
RC=-1: failure
referred from:
https://www.geeksforgeeks.org/mutex-lock-for-linux-thread-synchronization/

For spin lock, I found several useful links:
type __sync_lock_test_and_set (type *ptr, type value, ...)
void __sync_lock_release (type *ptr, ...)
Example:
volatile int lock = 0;
void *worker(void*)
{
    while (__sync_lock_test_and_set(&lock, 1));
    // critical section
    __sync_lock_release(&lock);
}
referred from:
https://stackoverflow.com/questions/49932746/what-is-the-gcc-builtin-for-an-atomic-set
https://gcc.gnu.org/onlinedocs/gcc/_005f_005fsync-Builtins.html

For compare and swap, and referred to:
https://stackoverflow.com/questions/22339466/how-compare-and-swap-works